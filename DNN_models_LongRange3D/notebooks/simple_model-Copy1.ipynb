{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "/users/mtaranov/local/anaconda2/lib/python2.7/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from dragonn.models import Model, SequenceDNN\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import Callback, EarlyStopping\n",
    "from keras.layers.core import (\n",
    "    Activation, Dense, Dropout, Flatten,\n",
    "    Permute, Reshape, TimeDistributedDense\n",
    ")\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.regularizers import l1\n",
    "\n",
    "from deeplift import keras_conversion as kc\n",
    "from deeplift.blobs import MxtsMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LongRangeDNN(SequenceDNN):\n",
    "    def __init__(self, num_features=10, num_nodes=2, use_deep_CNN=False,\n",
    "                  num_tasks=1, num_filters=25,\n",
    "                  num_filters_2=25, num_filters_3=25,\n",
    "                  L1=0, dropout=0.0, verbose=2):\n",
    "        self.num_features = num_features\n",
    "        self.num_nodes = num_nodes\n",
    "        self.input_shape = (1, num_features, num_nodes)\n",
    "        self.num_tasks = num_tasks\n",
    "        self.verbose = verbose\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Convolution2D(\n",
    "            nb_filter=num_filters, nb_row=num_features,\n",
    "            nb_col=1, activation='linear',\n",
    "            init='he_normal', input_shape=self.input_shape))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(Dropout(dropout))\n",
    "        if use_deep_CNN:\n",
    "            self.model.add(Convolution2D(\n",
    "                nb_filter=num_filters_2, nb_row=1,\n",
    "                nb_col=1, activation='relu',\n",
    "                init='he_normal', W_regularizer=l1(L1)))\n",
    "            self.model.add(Dropout(dropout))\n",
    "            self.model.add(Convolution2D(\n",
    "                nb_filter=num_filters_3, nb_row=1,\n",
    "                nb_col=1, activation='relu',\n",
    "                init='he_normal', W_regularizer=l1(L1)))\n",
    "            self.model.add(Dropout(dropout))\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(output_dim=25, activation='relu'))\n",
    "        self.model.add(Dense(output_dim=25, activation='relu'))\n",
    "        self.model.add(Dense(output_dim=25, activation='relu'))\n",
    "        self.model.add(Dense(output_dim=self.num_tasks))\n",
    "        self.model.add(Activation('sigmoid'))\n",
    "        self.model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "        self.train_losses = None\n",
    "        self.valid_losses = None\n",
    "    \n",
    "    def train(self, X, y, validation_data):\n",
    "        if y.dtype != bool:\n",
    "            assert len(np.unique(y)) == 2\n",
    "            y = y.astype(bool)\n",
    "        multitask = y.shape[1] > 1\n",
    "        if not multitask:\n",
    "            num_positives = y.sum()\n",
    "            num_sequences = len(y)\n",
    "            num_negatives = num_sequences - num_positives\n",
    "        self.callbacks = [EarlyStopping(monitor='val_loss', patience=10)]\n",
    "        if self.verbose >= 1:\n",
    "            self.callbacks.append(self.PrintMetrics(validation_data, self))\n",
    "            print('Training model...')\n",
    "        self.callbacks.append(self.LossHistory(X, y, validation_data, self))\n",
    "        self.model.fit(\n",
    "            X, y, batch_size=250, nb_epoch=100,\n",
    "            validation_data=validation_data,\n",
    "            class_weight={True: num_sequences / num_positives,\n",
    "                          False: num_sequences / num_negatives}\n",
    "            if not multitask else None,\n",
    "            callbacks=self.callbacks, verbose=self.verbose >= 2)\n",
    "        self.train_losses = self.callbacks[-1].train_losses\n",
    "        self.valid_losses = self.callbacks[-1].valid_losses\n",
    "        \n",
    "    def deeplift(self, X, batch_size=200):\n",
    "        \"\"\"\n",
    "        Returns (num_task, num_samples, input_shape) deeplift score array.\n",
    "        \"\"\"\n",
    "        # run deeplift\n",
    "        deeplift_model = kc.convert_sequential_model(\n",
    "            self.model, mxts_mode=MxtsMode.DeepLIFT)\n",
    "        target_contribs_func = deeplift_model.get_target_contribs_func(\n",
    "            find_scores_layer_idx=0)\n",
    "        return np.asarray([\n",
    "            target_contribs_func(task_idx=i, input_data_list=[X],\n",
    "                                 batch_size=batch_size, progress_update=10000)\n",
    "            for i in range(self.num_tasks)])\n",
    "\n",
    "\n",
    "def get_features(X_path):\n",
    "    # load data\n",
    "    X = np.load(X_path)\n",
    "    # reshape data\n",
    "    _, num_features, num_nodes = X.shape\n",
    "    return X.reshape(len(X), 1, num_features, num_nodes)\n",
    "\n",
    "    \n",
    "def get_labels(y_path):\n",
    "    # load data\n",
    "    y = np.load(y_path)\n",
    "    # reshape data\n",
    "    return y.reshape((len(y)), 1).astype(bool)\n",
    "\n",
    "def subsample_data(X, y, imbalance_ratio=10):\n",
    "    neg_indxs = np.where(y==False)[0]\n",
    "    pos_indxs = np.where(y==True)[0]\n",
    "    num_negatives = len(pos_indxs)*imbalance_ratio\n",
    "    y_subsampled = np.array([y[i] for i in np.concatenate((pos_indxs, neg_indxs[:num_negatives]))])\n",
    "    X_subsampled = np.array([X[i] for i in np.concatenate((pos_indxs, neg_indxs[:num_negatives]))])\n",
    "    \n",
    "    return (X_subsampled, y_subsampled)\n",
    "\n",
    "def normalize_features(X_train, X_valid, X_test, normalizer=MinMaxScaler):\n",
    "    # fit normalizer\n",
    "    normalizer = normalizer().fit(X_train[:, 0, :, 0])\n",
    "    # transform features\n",
    "    X_train[:, 0, :, 0] = normalizer.transform(X_train[:, 0, :, 0])\n",
    "    X_train[:, 0, :, 1] = normalizer.transform(X_train[:, 0, :, 1])\n",
    "    X_valid[:, 0, :, 0] = normalizer.transform(X_valid[:, 0, :, 0])\n",
    "    X_valid[:, 0, :, 1] = normalizer.transform(X_valid[:, 0, :, 1])\n",
    "    X_test[:, 0, :, 0] = normalizer.transform(X_test[:, 0, :, 0])\n",
    "    X_test[:, 0, :, 1] = normalizer.transform(X_test[:, 0, :, 1])\n",
    "    \n",
    "    return (X_train, X_valid, X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = get_features('/users/mtaranov/NN_all_data/train_set_upperTriangle_noDiag_reads.npy')\n",
    "y_train = get_labels('/users/mtaranov/NN_all_data/labels_train_upperTriangle_noDiag.npy')\n",
    "X_valid = get_features('/users/mtaranov/NN_all_data/vali_set_upperTriangle_noDiag_reads.npy')\n",
    "y_valid = get_labels('/users/mtaranov/NN_all_data/labels_vali_upperTriangle_noDiag.npy')\n",
    "X_test = get_features('/users/mtaranov/NN_all_data/test_set_upperTriangle_noDiag_reads.npy')\n",
    "y_test = get_labels('/users/mtaranov/NN_all_data/labels_test_upperTriangle_noDiag.npy')\n",
    "\"\"\"\n",
    "X_train = get_features(data_path+'train_set_upperTriangle_noDiag_reads.npy')\n",
    "y_train = get_labels(data_path+'labels_train_upperTriangle_noDiag.npy')\n",
    "X_valid = get_features(data_path+'vali_set_upperTriangle_noDiag_reads.npy')\n",
    "y_valid = get_labels(data_path+'labels_vali_upperTriangle_noDiag.npy')\n",
    "X_test = get_features(data_path+'test_set_upperTriangle_noDiag_reads.npy')\n",
    "y_test = get_labels(data_path+'labels_test_upperTriangle_noDiag.npy')\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "X_train = get_features('NN_datasets/train_set_all_reads.npy')\n",
    "y_train = get_labels('NN_datasets/labels_train_all.npy')\n",
    "X_valid = get_features('NN_datasets/vali_set_all_reads.npy')\n",
    "y_valid = get_labels('NN_datasets/labels_vali_all.npy')\n",
    "X_test = get_features('NN_datasets/test_set_all_reads.npy')\n",
    "y_test = get_labels('NN_datasets/labels_test_all.npy')\n",
    "\"\"\"\n",
    "X_train_subsampled, y_train_subsampled = subsample_data(X_train, y_train)\n",
    "X_valid_subsampled, y_valid_subsampled = subsample_data(X_valid, y_valid)\n",
    "X_test_subsampled, y_test_subsampled = subsample_data(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_scaled, X_valid_scaled, X_test_scaled = normalize_features(X_train, X_valid, X_test)\n",
    "X_train_normalized, X_valid_normalized, X_test_normalized = normalize_features(X_train, X_valid, X_test,\n",
    "                                                                               normalizer=StandardScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_normalized_subsampled, y_train_subsampled = subsample_data(X_train_normalized, y_train)\n",
    "X_valid_normalized_subsampled, y_valid_subsampled = subsample_data(X_valid_normalized, y_valid)\n",
    "X_test_normalized_subsampled, y_test_subsampled = subsample_data(X_test_normalized, y_test)\n",
    "\n",
    "X_train_scaled_subsampled, y_train_subsampled = subsample_data(X_train_scaled, y_train)\n",
    "X_valid_scaled_subsampled, y_valid_subsampled = subsample_data(X_valid_scaled, y_valid)\n",
    "X_test_scaled_subsampled, y_test_subsampled = subsample_data(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dnn_normalized = LongRangeDNN(num_features=11, use_deep_CNN=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Train on 121011 samples, validate on 74063 samples\n",
      "Epoch 1/100\n",
      "121011/121011 [==============================] - 3s - loss: 16.1181 - val_loss: 1.4653\n",
      "Epoch 0: validation loss: 1.465\n",
      "Balanced Accuracy: 50.00%\t auROC: 0.500\t auPRC: 0.545\t auPRG: 0.000\n",
      "Recall at 5%|10%|20% FDR: 0.0%|0.0%|0.0%\t Num Positives: 6733\t Num Negatives: 67330\t \n",
      "\n",
      "Epoch 2/100\n",
      "121011/121011 [==============================] - 3s - loss: 16.1181 - val_loss: 1.4653\n",
      "Epoch 1: validation loss: 1.465\n",
      "Balanced Accuracy: 50.00%\t auROC: 0.500\t auPRC: 0.545\t auPRG: 0.000\n",
      "Recall at 5%|10%|20% FDR: 0.0%|0.0%|0.0%\t Num Positives: 6733\t Num Negatives: 67330\t \n",
      "\n",
      "Epoch 3/100\n",
      "121011/121011 [==============================] - 2s - loss: 16.1181 - val_loss: 1.4653\n",
      "Epoch 2: validation loss: 1.465\n",
      "Balanced Accuracy: 50.00%\t auROC: 0.500\t auPRC: 0.545\t auPRG: 0.000\n",
      "Recall at 5%|10%|20% FDR: 0.0%|0.0%|0.0%\t Num Positives: 6733\t Num Negatives: 67330\t \n",
      "\n",
      "Epoch 4/100\n",
      "121011/121011 [==============================] - 3s - loss: 16.1181 - val_loss: 1.4653\n",
      "Epoch 3: validation loss: 1.465\n",
      "Balanced Accuracy: 50.00%\t auROC: 0.500\t auPRC: 0.545\t auPRG: 0.000\n",
      "Recall at 5%|10%|20% FDR: 0.0%|0.0%|0.0%\t Num Positives: 6733\t Num Negatives: 67330\t \n",
      "\n",
      "Epoch 5/100\n",
      "121011/121011 [==============================] - 3s - loss: 16.1181 - val_loss: 1.4653\n",
      "Epoch 4: validation loss: 1.465\n",
      "Balanced Accuracy: 50.00%\t auROC: 0.500\t auPRC: 0.545\t auPRG: 0.000\n",
      "Recall at 5%|10%|20% FDR: 0.0%|0.0%|0.0%\t Num Positives: 6733\t Num Negatives: 67330\t \n",
      "\n",
      "Epoch 6/100\n",
      "121011/121011 [==============================] - 2s - loss: 16.1181 - val_loss: 1.4653\n",
      "Epoch 5: validation loss: 1.465\n",
      "Balanced Accuracy: 50.00%\t auROC: 0.500\t auPRC: 0.545\t auPRG: 0.000\n",
      "Recall at 5%|10%|20% FDR: 0.0%|0.0%|0.0%\t Num Positives: 6733\t Num Negatives: 67330\t \n",
      "\n",
      "Epoch 7/100\n",
      "121011/121011 [==============================] - 2s - loss: 16.1181 - val_loss: 1.4653\n",
      "Epoch 6: validation loss: 1.465\n",
      "Balanced Accuracy: 50.00%\t auROC: 0.500\t auPRC: 0.545\t auPRG: 0.000\n",
      "Recall at 5%|10%|20% FDR: 0.0%|0.0%|0.0%\t Num Positives: 6733\t Num Negatives: 67330\t \n",
      "\n",
      "Epoch 8/100\n",
      "121011/121011 [==============================] - 3s - loss: 16.1181 - val_loss: 1.4653\n",
      "Epoch 7: validation loss: 1.465\n",
      "Balanced Accuracy: 50.00%\t auROC: 0.500\t auPRC: 0.545\t auPRG: 0.000\n",
      "Recall at 5%|10%|20% FDR: 0.0%|0.0%|0.0%\t Num Positives: 6733\t Num Negatives: 67330\t \n",
      "\n",
      "Epoch 9/100\n",
      "121011/121011 [==============================] - 2s - loss: 16.1181 - val_loss: 1.4653\n",
      "Epoch 8: validation loss: 1.465\n",
      "Balanced Accuracy: 50.00%\t auROC: 0.500\t auPRC: 0.545\t auPRG: 0.000\n",
      "Recall at 5%|10%|20% FDR: 0.0%|0.0%|0.0%\t Num Positives: 6733\t Num Negatives: 67330\t \n",
      "\n",
      "Epoch 10/100\n",
      "121011/121011 [==============================] - 2s - loss: 16.1181 - val_loss: 1.4653\n",
      "Epoch 9: validation loss: 1.465\n",
      "Balanced Accuracy: 50.00%\t auROC: 0.500\t auPRC: 0.545\t auPRG: 0.000\n",
      "Recall at 5%|10%|20% FDR: 0.0%|0.0%|0.0%\t Num Positives: 6733\t Num Negatives: 67330\t \n",
      "\n",
      "Epoch 11/100\n",
      "121011/121011 [==============================] - 2s - loss: 16.1181 - val_loss: 1.4653\n",
      "Epoch 10: validation loss: 1.465\n",
      "Balanced Accuracy: 50.00%\t auROC: 0.500\t auPRC: 0.545\t auPRG: 0.000\n",
      "Recall at 5%|10%|20% FDR: 0.0%|0.0%|0.0%\t Num Positives: 6733\t Num Negatives: 67330\t \n",
      "\n",
      "Epoch 12/100\n",
      "121011/121011 [==============================] - 2s - loss: 16.1181 - val_loss: 1.4653\n",
      "Epoch 11: validation loss: 1.465\n",
      "Balanced Accuracy: 50.00%\t auROC: 0.500\t auPRC: 0.545\t auPRG: 0.000\n",
      "Recall at 5%|10%|20% FDR: 0.0%|0.0%|0.0%\t Num Positives: 6733\t Num Negatives: 67330\t \n",
      "\n"
     ]
    }
   ],
   "source": [
    "validation_data = (X_valid_subsampled[:, :, :11, :], y_valid_subsampled)\n",
    "#validation_data = (X_test_normalized_subsampled[:, :, :10, :], y_test_subsampled)\n",
    "#validation_data = (X_test_subsampled[:, :, :10, :], y_test_subsampled)\n",
    "\n",
    "dnn_normalized.train(X_train_subsampled[:, :, :11, :], y_train_subsampled, validation_data)\n",
    "#dnn_normalized.train(X_test_normalized_subsampled[:, :, :10, :], y_test_subsampled, validation_data)\n",
    "#dnn_normalized.train(X_test_subsampled[:, :, :10, :], y_test_subsampled, validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(dnn_normalized.test(X_test[:, :, :10, :], y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_probs = dnn_normalized.predict(X_test_normalized[:, :, :10, :])\n",
    "np.save(\"model_predictions/test_set_upperTriangle_noDiag_reads_without_distances.npy\", pred_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(dnn_normalized.test(X_valid[:, :, :10, :], y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(dnn_normalized.test(X_train[:, :, :10, :], y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dnn_normalized.train(X_train_normalized[:, :, :10, :], y_train,\n",
    "                     (X_valid_normalized[:, :, :10, :], y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reads_normalized_full_result = dnn_normalized.test(X_valid_normalized[:, :, :10, :], y_valid)\n",
    "print(\"reads_normalized_full_result...\")\n",
    "print(reads_normalized_full_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reads_normalized_full_test_result = dnn_normalized.test(X_test_normalized[:, :, :10, :], y_test)\n",
    "print(\"reads_normalized_full_test_result...\")\n",
    "print(reads_normalized_full_test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_probs = dnn_normalized.predict(X_test_normalized[:, :, :10, :])\n",
    "np.save(\"model_predictions/test_set_upperTriangle_noDiag_reads_without_distances.npy\", pred_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dnn_scaled = LongRangeDNN(num_features=10, use_deep_CNN=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reads_scaled_validation_data = (X_valid_scaled_subsampled[:, :, :10, :], y_valid_subsampled)\n",
    "dnn_scaled.train(X_train_scaled_subsampled[:, :, :10, :], y_train_subsampled, reads_scaled_validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_scaled, X_test_scaled = normalize_features(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reads_scaled_full_test_result = dnn_scaled.test(X_test_scaled[:, :, :10, :], y_test)\n",
    "print(\"reads_scaled_full_test_result...\")\n",
    "print(reads_scaled_full_test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_normalized_2, X_valid_normalized_2 = normalize_features(X_train, X_valid, normalizer=StandardScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_subsampled_2, y_train_subsampled_2 = subsample_data(X_train_normalized_2, y_train)\n",
    "X_valid_subsampled_2, y_valid_subsampled_2 = subsample_data(X_valid_normalized_2, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dnn_normalized_2 = LongRangeDNN(num_features=10, use_deep_CNN=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "validation_data = (X_valid_subsampled_2[:, :, :10, :], y_valid_subsampled_2)\n",
    "dnn_normalized_2.train(X_train_subsampled_2[:, :, :10, :], y_train_subsampled_2, validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = dnn_normalized_2.predict(X_valid_subsampled_2[:, :, :10, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(preds>0.75).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_indxs = y_valid_subsampled_2==True\n",
    "pred_pos_indxs = preds > 0.5\n",
    "true_pos_indxs = pos_indxs*pred_pos_indxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "true_pos_indxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dl_scores = dnn_normalized_2.deeplift(X_valid_subsampled_2[true_pos_indxs[:,0], :, :10, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dl_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_, num_examples, _, num_features, num_nodes = dl_scores.shape\n",
    "concat_dl_scores = dl_scores[0, :, 0, :, :].reshape(num_examples, num_features*num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fit_pca = pca.fit(concat_dl_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "concat_dl_scores_reduced = pca.transform(concat_dl_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.scatter(concat_dl_scores_reduced[:, 0], concat_dl_scores_reduced[:, 1], c=clusters2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2).fit(concat_dl_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clusters2 = kmeans.predict(concat_dl_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tsne = TSNE(verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fit_tsne = tsne.fit(concat_dl_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "concat_dl_scores_tsne = fit_tsne.fit_transform(concat_dl_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(concat_dl_scores_tsne[:, 0], concat_dl_scores_tsne[:, 1], c=clusters2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
